{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport sqlite3","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-23T08:40:19.045041Z","iopub.execute_input":"2025-05-23T08:40:19.045301Z","iopub.status.idle":"2025-05-23T08:40:19.051512Z","shell.execute_reply.started":"2025-05-23T08:40:19.045281Z","shell.execute_reply":"2025-05-23T08:40:19.050787Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from tensorflow.keras.datasets import mnist\n\n# Load MNIST dataset (train and test sets)\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n\n# Normalize pixel values from 0-255 to 0-1 float range\ntrain_images = train_images.astype(np.float32) / 255.0\ntest_images = test_images.astype(np.float32) / 255.0\n\n# Flatten images from (28,28) to (784,1)\ntrain_images = train_images.reshape((-1, 28*28)).T  # shape (784, num_train)\ntest_images = test_images.reshape((-1, 28*28)).T    # shape (784, num_test)\n\n# One-hot encode the labels (0-9) into vectors of length 10\ndef one_hot_encode(labels, num_classes=10):\n    one_hot = np.zeros((num_classes, labels.size))\n    for idx, val in enumerate(labels):\n        one_hot[val, idx] = 1.0\n    return one_hot\n\ntrain_labels_oh = one_hot_encode(train_labels)  # shape (10, num_train)\ntest_labels_oh = one_hot_encode(test_labels)    # shape (10, num_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T08:40:21.150122Z","iopub.execute_input":"2025-05-23T08:40:21.150833Z","iopub.status.idle":"2025-05-23T08:40:35.843413Z","shell.execute_reply.started":"2025-05-23T08:40:21.150804Z","shell.execute_reply":"2025-05-23T08:40:35.842859Z"}},"outputs":[{"name":"stderr","text":"2025-05-23 08:40:22.957898: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747989623.169062      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747989623.228921      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"def sigmoid(z):\n    return 1/(1+np.exp(-z)) #  Converts input into a value between 0 and 1.\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T08:40:51.235412Z","iopub.execute_input":"2025-05-23T08:40:51.236500Z","iopub.status.idle":"2025-05-23T08:40:51.239962Z","shell.execute_reply.started":"2025-05-23T08:40:51.236462Z","shell.execute_reply":"2025-05-23T08:40:51.239239Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def softmax(z):\n    exp_z = np.exp(z - np.max(z))  # for numerical stability\n    return exp_z / np.sum(exp_z, axis=0, keepdims=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T08:40:57.659559Z","iopub.execute_input":"2025-05-23T08:40:57.659877Z","iopub.status.idle":"2025-05-23T08:40:57.663798Z","shell.execute_reply.started":"2025-05-23T08:40:57.659855Z","shell.execute_reply":"2025-05-23T08:40:57.663157Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"no_of_pixels= 784\nhidden_layer_nodes=30\noutput_nodes=10 ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T08:40:59.079960Z","iopub.execute_input":"2025-05-23T08:40:59.080669Z","iopub.status.idle":"2025-05-23T08:40:59.083887Z","shell.execute_reply.started":"2025-05-23T08:40:59.080635Z","shell.execute_reply":"2025-05-23T08:40:59.083155Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"np.random.seed(42)\n# makes debugging and comparing to find ideal values easier\nw1= np.random.randn(hidden_layer_nodes,no_of_pixels)\nb1= np.random.randn(hidden_layer_nodes,1)\nw2= np.random.randn(output_nodes,hidden_layer_nodes)\nb2= np.random.randn(output_nodes,1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T08:41:01.396183Z","iopub.execute_input":"2025-05-23T08:41:01.396712Z","iopub.status.idle":"2025-05-23T08:41:01.401312Z","shell.execute_reply.started":"2025-05-23T08:41:01.396680Z","shell.execute_reply":"2025-05-23T08:41:01.400665Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def cost(y,a2): #measures how far away the predicted vaue label is from actual label\n    return 0.5*np.sum(y-a2)**2 \n                        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T08:41:03.560205Z","iopub.execute_input":"2025-05-23T08:41:03.560744Z","iopub.status.idle":"2025-05-23T08:41:03.564292Z","shell.execute_reply.started":"2025-05-23T08:41:03.560721Z","shell.execute_reply":"2025-05-23T08:41:03.563648Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def sigmoid_prime(z):\n    return sigmoid(z) * (1 - sigmoid(z)) # derivative of sigmoid \n                                         # used to compute the gradient of the loss function with respect to the weights of the network,","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T08:41:05.576797Z","iopub.execute_input":"2025-05-23T08:41:05.577085Z","iopub.status.idle":"2025-05-23T08:41:05.581158Z","shell.execute_reply.started":"2025-05-23T08:41:05.577065Z","shell.execute_reply":"2025-05-23T08:41:05.580321Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def training(x, y):\n    global w1, b1, w2, b2\n    eta = 0.01  # learning rate\n\n    # Forward pass\n    z1 = np.dot(w1, x) + b1\n    a1 = sigmoid(z1)\n\n    z2 = np.dot(w2, a1) + b2\n    a2 = sigmoid(z2)\n\n    # Backward pass (compute gradients)\n    delta2 = (a2 - y) * sigmoid_prime(z2)\n    dw2 = np.dot(delta2, a1.T)  \n    db2 = delta2\n\n    delta1 = np.dot(w2.T, delta2) * sigmoid_prime(z1)\n    dw1 = np.dot(delta1, x.T)   \n    db1 = delta1\n\n    # Updates weights and biases\n    w2 -= eta * dw2\n    b2 -= eta * db2\n    w1 -= eta * dw1\n    b1 -= eta * db1\n\n    # Returns cost\n    cost = np.sum((a2 - y) ** 2) / 2\n    return cost\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T08:41:07.110517Z","iopub.execute_input":"2025-05-23T08:41:07.111274Z","iopub.status.idle":"2025-05-23T08:41:07.116304Z","shell.execute_reply.started":"2025-05-23T08:41:07.111250Z","shell.execute_reply":"2025-05-23T08:41:07.115644Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score # used to check how good its at predicitng","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T08:41:10.965687Z","iopub.execute_input":"2025-05-23T08:41:10.966363Z","iopub.status.idle":"2025-05-23T08:41:11.072054Z","shell.execute_reply.started":"2025-05-23T08:41:10.966340Z","shell.execute_reply":"2025-05-23T08:41:11.071527Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"#main implementation code for number recognition algorithm\n#contains code for main 4 metrics for checking performance for  machine leaning\n\ndef train_and_evaluate(train_images, train_labels_oh, test_images, test_labels_oh, epochs=100, samples=3000):\n    conn = sqlite3.connect(\"training_log.db\")    # Connect to SQLite database (or create it if it doesn't exist)\n\n    cursor = conn.cursor()\n        # Create the table to store performance metrics (if it doesn’t already exist)\n\n    cursor.execute(\"\"\"\n    CREATE TABLE IF NOT EXISTS training_log (\n        epoch INTEGER,\n        avg_cost REAL,\n        accuracy REAL,\n        precision REAL,\n        recall REAL,\n        f1 REAL\n    )\n    \"\"\")\n    conn.commit()\n\n    for epoch in range(epochs):\n        total_cost = 0\n        for i in range(samples):\n            x = train_images[:, i].reshape(-1, 1)\n            y = train_labels_oh[:, i].reshape(-1, 1)  # Get the corresponding one-hot encoded label and reshape it into a column vector\n\n            c = training(x, y)\n            total_cost += c\n\n        avg_cost = total_cost / samples\n\n    # Evaluation on test\n        y_true = []\n        y_pred = []\n\n        for i in range(test_images.shape[1]):\n            x_test = test_images[:, i].reshape(-1, 1) \n            y_actual = np.argmax(test_labels_oh[:, i])  # Get the corresponding one-hot encoded label and reshape it into a column vector\n\n\n        # Inline prediction\n            z1 = np.dot(w1, x_test) + b1\n            a1 = sigmoid(z1)\n            z2 = np.dot(w2, a1) + b2\n            a2 = softmax(z2) #converts the final layer's raw outputs (called logits) into probabilities which sum to 1, highest probabilty for the output ndoes is taken as predicted class\n\n            y_hat = np.argmax(a2) #Returns the index of the largest value in the array, we use it as the index corresponds to the numbers predicted\n\n            y_true.append(y_actual)\n            y_pred.append(y_hat)\n            \n    # Calculate the performance metrics\n\n        accuracy = np.mean(np.array(y_true) == np.array(y_pred))\n        precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n        recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n        f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n\n        print(f\"Epoch {epoch+1} — Cost: {avg_cost:.4f}, Acc: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n\n     # Store results in database\n        cursor.execute(\"\"\"\n            INSERT INTO training_log (epoch, avg_cost, accuracy, precision, recall, f1)\n            VALUES (?, ?, ?, ?, ?, ?)\n        \"\"\", (epoch+1, avg_cost, accuracy, precision, recall, f1))\n        conn.commit()\n\n    conn.close()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T08:41:13.511069Z","iopub.execute_input":"2025-05-23T08:41:13.511632Z","iopub.status.idle":"2025-05-23T08:41:13.519740Z","shell.execute_reply.started":"2025-05-23T08:41:13.511596Z","shell.execute_reply":"2025-05-23T08:41:13.519072Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"train_and_evaluate(train_images, train_labels_oh, test_images, test_labels_oh, epochs=100, samples=5000)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T08:41:19.644155Z","iopub.execute_input":"2025-05-23T08:41:19.644428Z","iopub.status.idle":"2025-05-23T08:43:18.883932Z","shell.execute_reply.started":"2025-05-23T08:41:19.644409Z","shell.execute_reply":"2025-05-23T08:43:18.883237Z"}},"outputs":[{"name":"stdout","text":"Epoch 1 — Cost: 0.6036, Acc: 0.2779, Precision: 0.1708, Recall: 0.2668, F1: 0.1967\nEpoch 2 — Cost: 0.4288, Acc: 0.3129, Precision: 0.1822, Recall: 0.3013, F1: 0.2118\nEpoch 3 — Cost: 0.4109, Acc: 0.3358, Precision: 0.1952, Recall: 0.3240, F1: 0.2288\nEpoch 4 — Cost: 0.3971, Acc: 0.3588, Precision: 0.2134, Recall: 0.3466, F1: 0.2499\nEpoch 5 — Cost: 0.3854, Acc: 0.3806, Precision: 0.2294, Recall: 0.3679, F1: 0.2727\nEpoch 6 — Cost: 0.3746, Acc: 0.4068, Precision: 0.2552, Recall: 0.3934, F1: 0.2981\nEpoch 7 — Cost: 0.3637, Acc: 0.4287, Precision: 0.2872, Recall: 0.4148, F1: 0.3180\nEpoch 8 — Cost: 0.3527, Acc: 0.4435, Precision: 0.3156, Recall: 0.4292, F1: 0.3316\nEpoch 9 — Cost: 0.3425, Acc: 0.4571, Precision: 0.3339, Recall: 0.4426, F1: 0.3449\nEpoch 10 — Cost: 0.3332, Acc: 0.4719, Precision: 0.3524, Recall: 0.4572, F1: 0.3631\nEpoch 11 — Cost: 0.3239, Acc: 0.4992, Precision: 0.3835, Recall: 0.4842, F1: 0.4002\nEpoch 12 — Cost: 0.3128, Acc: 0.5266, Precision: 0.4069, Recall: 0.5115, F1: 0.4290\nEpoch 13 — Cost: 0.3020, Acc: 0.5428, Precision: 0.4267, Recall: 0.5275, F1: 0.4443\nEpoch 14 — Cost: 0.2930, Acc: 0.5556, Precision: 0.4488, Recall: 0.5402, F1: 0.4559\nEpoch 15 — Cost: 0.2854, Acc: 0.5622, Precision: 0.4851, Recall: 0.5468, F1: 0.4630\nEpoch 16 — Cost: 0.2787, Acc: 0.5712, Precision: 0.4999, Recall: 0.5558, F1: 0.4739\nEpoch 17 — Cost: 0.2727, Acc: 0.5795, Precision: 0.4646, Recall: 0.5641, F1: 0.4846\nEpoch 18 — Cost: 0.2671, Acc: 0.5881, Precision: 0.4725, Recall: 0.5727, F1: 0.4964\nEpoch 19 — Cost: 0.2618, Acc: 0.5967, Precision: 0.4935, Recall: 0.5815, F1: 0.5089\nEpoch 20 — Cost: 0.2566, Acc: 0.6039, Precision: 0.5351, Recall: 0.5888, F1: 0.5189\nEpoch 21 — Cost: 0.2517, Acc: 0.6103, Precision: 0.5270, Recall: 0.5953, F1: 0.5268\nEpoch 22 — Cost: 0.2470, Acc: 0.6176, Precision: 0.5354, Recall: 0.6027, F1: 0.5359\nEpoch 23 — Cost: 0.2426, Acc: 0.6251, Precision: 0.5500, Recall: 0.6103, F1: 0.5453\nEpoch 24 — Cost: 0.2382, Acc: 0.6313, Precision: 0.5569, Recall: 0.6166, F1: 0.5535\nEpoch 25 — Cost: 0.2338, Acc: 0.6385, Precision: 0.5780, Recall: 0.6240, F1: 0.5657\nEpoch 26 — Cost: 0.2293, Acc: 0.6454, Precision: 0.6002, Recall: 0.6312, F1: 0.5785\nEpoch 27 — Cost: 0.2241, Acc: 0.6620, Precision: 0.6178, Recall: 0.6484, F1: 0.6050\nEpoch 28 — Cost: 0.2172, Acc: 0.6793, Precision: 0.6380, Recall: 0.6665, F1: 0.6302\nEpoch 29 — Cost: 0.2082, Acc: 0.6939, Precision: 0.6504, Recall: 0.6818, F1: 0.6481\nEpoch 30 — Cost: 0.1989, Acc: 0.7066, Precision: 0.6609, Recall: 0.6949, F1: 0.6618\nEpoch 31 — Cost: 0.1911, Acc: 0.7157, Precision: 0.6678, Recall: 0.7042, F1: 0.6713\nEpoch 32 — Cost: 0.1848, Acc: 0.7230, Precision: 0.6836, Recall: 0.7117, F1: 0.6795\nEpoch 33 — Cost: 0.1797, Acc: 0.7279, Precision: 0.6867, Recall: 0.7167, F1: 0.6845\nEpoch 34 — Cost: 0.1753, Acc: 0.7340, Precision: 0.6964, Recall: 0.7229, F1: 0.6910\nEpoch 35 — Cost: 0.1715, Acc: 0.7397, Precision: 0.7073, Recall: 0.7288, F1: 0.6977\nEpoch 36 — Cost: 0.1680, Acc: 0.7445, Precision: 0.7203, Recall: 0.7337, F1: 0.7035\nEpoch 37 — Cost: 0.1647, Acc: 0.7492, Precision: 0.7305, Recall: 0.7386, F1: 0.7100\nEpoch 38 — Cost: 0.1616, Acc: 0.7531, Precision: 0.7386, Recall: 0.7427, F1: 0.7163\nEpoch 39 — Cost: 0.1585, Acc: 0.7584, Precision: 0.7440, Recall: 0.7484, F1: 0.7250\nEpoch 40 — Cost: 0.1553, Acc: 0.7625, Precision: 0.7509, Recall: 0.7529, F1: 0.7332\nEpoch 41 — Cost: 0.1521, Acc: 0.7697, Precision: 0.7598, Recall: 0.7607, F1: 0.7452\nEpoch 42 — Cost: 0.1488, Acc: 0.7748, Precision: 0.7658, Recall: 0.7663, F1: 0.7542\nEpoch 43 — Cost: 0.1456, Acc: 0.7814, Precision: 0.7749, Recall: 0.7737, F1: 0.7649\nEpoch 44 — Cost: 0.1424, Acc: 0.7876, Precision: 0.7827, Recall: 0.7805, F1: 0.7741\nEpoch 45 — Cost: 0.1395, Acc: 0.7925, Precision: 0.7880, Recall: 0.7858, F1: 0.7808\nEpoch 46 — Cost: 0.1366, Acc: 0.7962, Precision: 0.7918, Recall: 0.7899, F1: 0.7858\nEpoch 47 — Cost: 0.1340, Acc: 0.8008, Precision: 0.7967, Recall: 0.7949, F1: 0.7916\nEpoch 48 — Cost: 0.1314, Acc: 0.8046, Precision: 0.8007, Recall: 0.7989, F1: 0.7963\nEpoch 49 — Cost: 0.1290, Acc: 0.8076, Precision: 0.8037, Recall: 0.8023, F1: 0.8001\nEpoch 50 — Cost: 0.1266, Acc: 0.8110, Precision: 0.8071, Recall: 0.8059, F1: 0.8040\nEpoch 51 — Cost: 0.1244, Acc: 0.8147, Precision: 0.8110, Recall: 0.8099, F1: 0.8084\nEpoch 52 — Cost: 0.1223, Acc: 0.8177, Precision: 0.8143, Recall: 0.8131, F1: 0.8119\nEpoch 53 — Cost: 0.1202, Acc: 0.8196, Precision: 0.8163, Recall: 0.8152, F1: 0.8141\nEpoch 54 — Cost: 0.1182, Acc: 0.8215, Precision: 0.8181, Recall: 0.8172, F1: 0.8161\nEpoch 55 — Cost: 0.1164, Acc: 0.8234, Precision: 0.8201, Recall: 0.8192, F1: 0.8183\nEpoch 56 — Cost: 0.1146, Acc: 0.8249, Precision: 0.8216, Recall: 0.8208, F1: 0.8199\nEpoch 57 — Cost: 0.1129, Acc: 0.8272, Precision: 0.8241, Recall: 0.8232, F1: 0.8224\nEpoch 58 — Cost: 0.1112, Acc: 0.8296, Precision: 0.8266, Recall: 0.8258, F1: 0.8251\nEpoch 59 — Cost: 0.1097, Acc: 0.8312, Precision: 0.8283, Recall: 0.8274, F1: 0.8268\nEpoch 60 — Cost: 0.1082, Acc: 0.8326, Precision: 0.8297, Recall: 0.8289, F1: 0.8283\nEpoch 61 — Cost: 0.1068, Acc: 0.8343, Precision: 0.8315, Recall: 0.8307, F1: 0.8302\nEpoch 62 — Cost: 0.1055, Acc: 0.8348, Precision: 0.8320, Recall: 0.8313, F1: 0.8308\nEpoch 63 — Cost: 0.1042, Acc: 0.8364, Precision: 0.8336, Recall: 0.8330, F1: 0.8325\nEpoch 64 — Cost: 0.1029, Acc: 0.8380, Precision: 0.8352, Recall: 0.8346, F1: 0.8342\nEpoch 65 — Cost: 0.1017, Acc: 0.8389, Precision: 0.8361, Recall: 0.8355, F1: 0.8351\nEpoch 66 — Cost: 0.1006, Acc: 0.8398, Precision: 0.8370, Recall: 0.8365, F1: 0.8360\nEpoch 67 — Cost: 0.0995, Acc: 0.8406, Precision: 0.8378, Recall: 0.8373, F1: 0.8368\nEpoch 68 — Cost: 0.0984, Acc: 0.8416, Precision: 0.8388, Recall: 0.8383, F1: 0.8379\nEpoch 69 — Cost: 0.0974, Acc: 0.8429, Precision: 0.8401, Recall: 0.8396, F1: 0.8392\nEpoch 70 — Cost: 0.0964, Acc: 0.8437, Precision: 0.8409, Recall: 0.8404, F1: 0.8400\nEpoch 71 — Cost: 0.0955, Acc: 0.8444, Precision: 0.8416, Recall: 0.8412, F1: 0.8407\nEpoch 72 — Cost: 0.0945, Acc: 0.8449, Precision: 0.8420, Recall: 0.8417, F1: 0.8412\nEpoch 73 — Cost: 0.0936, Acc: 0.8460, Precision: 0.8431, Recall: 0.8428, F1: 0.8424\nEpoch 74 — Cost: 0.0928, Acc: 0.8470, Precision: 0.8441, Recall: 0.8438, F1: 0.8434\nEpoch 75 — Cost: 0.0919, Acc: 0.8484, Precision: 0.8456, Recall: 0.8453, F1: 0.8449\nEpoch 76 — Cost: 0.0911, Acc: 0.8490, Precision: 0.8461, Recall: 0.8459, F1: 0.8455\nEpoch 77 — Cost: 0.0903, Acc: 0.8501, Precision: 0.8472, Recall: 0.8470, F1: 0.8466\nEpoch 78 — Cost: 0.0895, Acc: 0.8510, Precision: 0.8481, Recall: 0.8479, F1: 0.8475\nEpoch 79 — Cost: 0.0887, Acc: 0.8519, Precision: 0.8491, Recall: 0.8489, F1: 0.8485\nEpoch 80 — Cost: 0.0880, Acc: 0.8524, Precision: 0.8496, Recall: 0.8494, F1: 0.8490\nEpoch 81 — Cost: 0.0872, Acc: 0.8534, Precision: 0.8507, Recall: 0.8505, F1: 0.8501\nEpoch 82 — Cost: 0.0865, Acc: 0.8542, Precision: 0.8514, Recall: 0.8513, F1: 0.8509\nEpoch 83 — Cost: 0.0858, Acc: 0.8543, Precision: 0.8516, Recall: 0.8514, F1: 0.8510\nEpoch 84 — Cost: 0.0851, Acc: 0.8548, Precision: 0.8521, Recall: 0.8519, F1: 0.8515\nEpoch 85 — Cost: 0.0845, Acc: 0.8548, Precision: 0.8521, Recall: 0.8519, F1: 0.8516\nEpoch 86 — Cost: 0.0838, Acc: 0.8554, Precision: 0.8527, Recall: 0.8525, F1: 0.8522\nEpoch 87 — Cost: 0.0832, Acc: 0.8561, Precision: 0.8534, Recall: 0.8533, F1: 0.8529\nEpoch 88 — Cost: 0.0825, Acc: 0.8563, Precision: 0.8537, Recall: 0.8535, F1: 0.8532\nEpoch 89 — Cost: 0.0819, Acc: 0.8581, Precision: 0.8555, Recall: 0.8553, F1: 0.8550\nEpoch 90 — Cost: 0.0813, Acc: 0.8586, Precision: 0.8560, Recall: 0.8558, F1: 0.8555\nEpoch 91 — Cost: 0.0807, Acc: 0.8592, Precision: 0.8566, Recall: 0.8564, F1: 0.8561\nEpoch 92 — Cost: 0.0801, Acc: 0.8601, Precision: 0.8575, Recall: 0.8574, F1: 0.8571\nEpoch 93 — Cost: 0.0796, Acc: 0.8602, Precision: 0.8575, Recall: 0.8575, F1: 0.8572\nEpoch 94 — Cost: 0.0790, Acc: 0.8607, Precision: 0.8581, Recall: 0.8580, F1: 0.8577\nEpoch 95 — Cost: 0.0785, Acc: 0.8610, Precision: 0.8584, Recall: 0.8583, F1: 0.8580\nEpoch 96 — Cost: 0.0779, Acc: 0.8614, Precision: 0.8588, Recall: 0.8587, F1: 0.8584\nEpoch 97 — Cost: 0.0774, Acc: 0.8612, Precision: 0.8586, Recall: 0.8585, F1: 0.8582\nEpoch 98 — Cost: 0.0769, Acc: 0.8622, Precision: 0.8596, Recall: 0.8596, F1: 0.8593\nEpoch 99 — Cost: 0.0764, Acc: 0.8625, Precision: 0.8600, Recall: 0.8599, F1: 0.8596\nEpoch 100 — Cost: 0.0759, Acc: 0.8629, Precision: 0.8604, Recall: 0.8603, F1: 0.8600\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef predict_single_image(images, labels_onehot, index):  #seperate function for checking individual test data (motly just for asthetics as its  already run through test data )\n    global w1, b1, w2, b2\n\n    x = images[:, index].reshape(-1, 1)\n    y = labels_onehot[:, index].reshape(-1, 1)\n\n # Feedforward manually\n    z1 = np.dot(w1, x) + b1\n    a1 = sigmoid(z1)\n    z2 = np.dot(w2, a1) + b2\n    a2 = sigmoid(z2)\n\n    predicted_digit = np.argmax(a2)\n    true_digit = np.argmax(y)\n\n    print(f\"Predicted digit: {predicted_digit}\")\n    print(f\"True digit: {true_digit}\")\n\n    plt.imshow(x.reshape(28, 28), cmap='gray')\n    plt.title(f\"True: {true_digit} | Predicted: {predicted_digit}\")\n    plt.axis('off')\n    plt.show()\n\n    return predicted_digit, true_digit\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T08:43:35.732185Z","iopub.execute_input":"2025-05-23T08:43:35.732451Z","iopub.status.idle":"2025-05-23T08:43:35.737994Z","shell.execute_reply.started":"2025-05-23T08:43:35.732434Z","shell.execute_reply":"2025-05-23T08:43:35.737152Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"predict_single_image(test_images, test_labels_oh, 90)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T08:45:23.611282Z","iopub.execute_input":"2025-05-23T08:45:23.611583Z","iopub.status.idle":"2025-05-23T08:45:23.693785Z","shell.execute_reply.started":"2025-05-23T08:45:23.611561Z","shell.execute_reply":"2025-05-23T08:45:23.692582Z"}},"outputs":[{"name":"stdout","text":"Predicted digit: 3\nTrue digit: 3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"(3, 3)"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"import pandas as pd\n\n# Connects to the database\nconn = sqlite3.connect(\"training_log.db\")\n\n# Loads the entire table into a Pandas DataFrame\ndf = pd.read_sql_query(\"SELECT * FROM training_log\", conn)\n\n# Closes the connection\nconn.close()\n\n# Displays the entire DataFrame\npd.set_option('display.max_rows', None)  # Shows all rows\npd.set_option('display.max_columns', None)  # Shows all columns\npd.set_option('display.width', None)  \npd.set_option('display.colheader_justify', 'center')\n\nprint(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T08:46:31.884919Z","iopub.execute_input":"2025-05-23T08:46:31.885472Z","iopub.status.idle":"2025-05-23T08:46:31.907330Z","shell.execute_reply.started":"2025-05-23T08:46:31.885451Z","shell.execute_reply":"2025-05-23T08:46:31.906798Z"}},"outputs":[{"name":"stdout","text":"    epoch  avg_cost  accuracy  precision   recall      f1   \n0      1   0.603579   0.2779   0.170816   0.266784  0.196688\n1      2   0.428847   0.3129   0.182235   0.301333  0.211761\n2      3   0.410929   0.3358   0.195193   0.324009  0.228787\n3      4   0.397078   0.3588   0.213390   0.346633  0.249939\n4      5   0.385418   0.3806   0.229412   0.367930  0.272672\n5      6   0.374573   0.4068   0.255187   0.393401  0.298093\n6      7   0.363669   0.4287   0.287244   0.414783  0.318018\n7      8   0.352745   0.4435   0.315625   0.429248  0.331603\n8      9   0.342491   0.4571   0.333861   0.442628  0.344917\n9     10   0.333228   0.4719   0.352359   0.457205  0.363124\n10    11   0.323917   0.4992   0.383492   0.484228  0.400230\n11    12   0.312809   0.5266   0.406852   0.511451  0.428965\n12    13   0.302004   0.5428   0.426711   0.527486  0.444258\n13    14   0.292982   0.5556   0.448769   0.540221  0.455907\n14    15   0.285353   0.5622   0.485125   0.546782  0.463041\n15    16   0.278709   0.5712   0.499880   0.555788  0.473923\n16    17   0.272721   0.5795   0.464597   0.564095  0.484648\n17    18   0.267132   0.5881   0.472475   0.572739  0.496433\n18    19   0.261772   0.5967   0.493456   0.581450  0.508873\n19    20   0.256618   0.6039   0.535141   0.588780  0.518852\n20    21   0.251714   0.6103   0.527014   0.595252  0.526814\n21    22   0.247044   0.6176   0.535363   0.602667  0.535943\n22    23   0.242557   0.6251   0.550046   0.610269  0.545252\n23    24   0.238183   0.6313   0.556866   0.616573  0.553470\n24    25   0.233819   0.6385   0.577994   0.624026  0.565667\n25    26   0.229264   0.6454   0.600181   0.631192  0.578461\n26    27   0.224052   0.6620   0.617831   0.648376  0.604998\n27    28   0.217226   0.6793   0.638019   0.666499  0.630166\n28    29   0.208231   0.6939   0.650401   0.681772  0.648069\n29    30   0.198928   0.7066   0.660886   0.694873  0.661835\n30    31   0.191057   0.7157   0.667774   0.704217  0.671280\n31    32   0.184789   0.7230   0.683579   0.711748  0.679477\n32    33   0.179681   0.7279   0.686685   0.716738  0.684495\n33    34   0.175335   0.7340   0.696408   0.722948  0.690977\n34    35   0.171498   0.7397   0.707258   0.728796  0.697671\n35    36   0.168006   0.7445   0.720349   0.733748  0.703470\n36    37   0.164738   0.7492   0.730549   0.738610  0.710022\n37    38   0.161590   0.7531   0.738629   0.742745  0.716262\n38    39   0.158472   0.7584   0.744028   0.748371  0.724979\n39    40   0.155307   0.7625   0.750900   0.752887  0.733163\n40    41   0.152066   0.7697   0.759821   0.760687  0.745167\n41    42   0.148787   0.7748   0.765797   0.766348  0.754174\n42    43   0.145551   0.7814   0.774857   0.773660  0.764942\n43    44   0.142428   0.7876   0.782674   0.780465  0.774052\n44    45   0.139455   0.7925   0.787987   0.785844  0.780838\n45    46   0.136638   0.7962   0.791765   0.789881  0.785763\n46    47   0.133964   0.8008   0.796696   0.794855  0.791586\n47    48   0.131417   0.8046   0.800681   0.798950  0.796266\n48    49   0.128982   0.8076   0.803735   0.802256  0.800108\n49    50   0.126647   0.8110   0.807110   0.805870  0.803976\n50    51   0.124406   0.8147   0.811046   0.809900  0.808427\n51    52   0.122258   0.8177   0.814293   0.813098  0.811863\n52    53   0.120200   0.8196   0.816270   0.815187  0.814125\n53    54   0.118232   0.8215   0.818108   0.817165  0.816143\n54    55   0.116354   0.8234   0.820103   0.819222  0.818328\n55    56   0.114564   0.8249   0.821576   0.820791  0.819927\n56    57   0.112859   0.8272   0.824102   0.823186  0.822407\n57    58   0.111235   0.8296   0.826579   0.825761  0.825081\n58    59   0.109687   0.8312   0.828270   0.827446  0.826832\n59    60   0.108212   0.8326   0.829714   0.828898  0.828308\n60    61   0.106803   0.8343   0.831462   0.830729  0.830199\n61    62   0.105457   0.8348   0.831998   0.831285  0.830786\n62    63   0.104167   0.8364   0.833606   0.832971  0.832498\n63    64   0.102931   0.8380   0.835177   0.834624  0.834198\n64    65   0.101743   0.8389   0.836124   0.835545  0.835107\n65    66   0.100601   0.8398   0.836988   0.836458  0.836008\n66    67   0.099501   0.8406   0.837785   0.837292  0.836846\n67    68   0.098440   0.8416   0.838760   0.838343  0.837889\n68    69   0.097416   0.8429   0.840106   0.839620  0.839198\n69    70   0.096425   0.8437   0.840890   0.840439  0.840012\n70    71   0.095467   0.8444   0.841567   0.841150  0.840741\n71    72   0.094538   0.8449   0.842047   0.841658  0.841239\n72    73   0.093638   0.8460   0.843148   0.842764  0.842361\n73    74   0.092763   0.8470   0.844139   0.843788  0.843380\n74    75   0.091913   0.8484   0.845555   0.845264  0.844873\n75    76   0.091086   0.8490   0.846133   0.845873  0.845462\n76    77   0.090280   0.8501   0.847215   0.846983  0.846576\n77    78   0.089495   0.8510   0.848144   0.847936  0.847532\n78    79   0.088729   0.8519   0.849100   0.848879  0.848487\n79    80   0.087980   0.8524   0.849598   0.849386  0.849000\n80    81   0.087249   0.8534   0.850666   0.850472  0.850118\n81    82   0.086534   0.8542   0.851449   0.851295  0.850942\n82    83   0.085834   0.8543   0.851587   0.851387  0.851043\n83    84   0.085148   0.8548   0.852090   0.851897  0.851549\n84    85   0.084477   0.8548   0.852101   0.851901  0.851562\n85    86   0.083820   0.8554   0.852686   0.852520  0.852177\n86    87   0.083176   0.8561   0.853448   0.853259  0.852935\n87    88   0.082544   0.8563   0.853678   0.853464  0.853154\n88    89   0.081926   0.8581   0.855492   0.855316  0.855020\n89    90   0.081319   0.8586   0.856001   0.855826  0.855521\n90    91   0.080725   0.8592   0.856601   0.856449  0.856147\n91    92   0.080142   0.8601   0.857485   0.857376  0.857077\n92    93   0.079570   0.8602   0.857539   0.857467  0.857156\n93    94   0.079010   0.8607   0.858069   0.857985  0.857675\n94    95   0.078459   0.8610   0.858408   0.858320  0.858016\n95    96   0.077919   0.8614   0.858819   0.858737  0.858435\n96    97   0.077389   0.8612   0.858595   0.858541  0.858233\n97    98   0.076869   0.8622   0.859634   0.859566  0.859260\n98    99   0.076357   0.8625   0.859983   0.859912  0.859628\n99   100   0.075855   0.8629   0.860399   0.860325  0.860047\n","output_type":"stream"}],"execution_count":23}]}